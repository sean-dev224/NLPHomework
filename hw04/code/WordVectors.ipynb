{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors\n",
    "\n",
    "This assignment is comprised of three parts:\n",
    "\n",
    "1. **Theory**: Prove simple properties of cosine similarity. Prove that using sums of word vectors as phrase embeddings is problematic.\n",
    "2. **Implementation**: You will  experiment with sparse and dense vector representations of words.\n",
    "3. **Classification**: You will use LLM embeddings or moving reviews to train a sentiment analyzer.\n",
    "\n",
    "Word Vectors are often used as a fundamental component for downstream NLP tasks, e.g. question answering, text generation, translation, etc., so it is important to build some intuitions as to their strengths and weaknesses. Here, you will explore two types of word vectors:\n",
    "\n",
    "- those derived from *co-occurrence matrices*, and \n",
    "\n",
    "- those derived via *word2vec*. \n",
    "\n",
    "**Note on Terminology:** The terms \"word vectors\" and \"word embeddings\" are often used interchangeably. The term \"embedding\" refers to the fact that we are encoding aspects of a word's meaning in a lower dimensional space. As [Wikipedia](https://en.wikipedia.org/wiki/Word_embedding) states, \"*conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with a much lower dimension*\".\n",
    "\n",
    "Before getting started with the implementation, install the gensim library:\n",
    "\n",
    "```sh\n",
    "pip install --upgrade gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sean Devlin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\"> Submission Instructions</font>\n",
    "\n",
    "1. Click the Save button at the top of the Jupyter Notebook.\n",
    "2. Please make sure to have entered your name above.\n",
    "3. Select Cell -> All Output -> Clear. This will clear all the outputs from all cells (but will keep the content of ll cells). \n",
    "4. Select Cell -> Run All. This will run all the cells in order, and will take several minutes.\n",
    "5. Once you've rerun everything, select File -> Download as -> PDF via LaTeX and download a .pdf version showing the code and the output of all cells, and save it in the same folder that contains the notebook file .ipynb.\n",
    "6. Look at the PDF file and make sure all your solutions are there, displayed correctly. The PDF is the only thing we will see when grading!\n",
    "7. Submit **both** your PDF and notebook on Canvas. **Do not submit the data folder!**\n",
    "8. Verify your Canvas submission contains the correct files by downloading them after posting them on Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of cosine similarity (20p)\n",
    "\n",
    "1. Prove that doubling the length of a vector $\\mathbf{u}$ does not change its cosine similarity with any other vector $\\mathbf{v}$, i.e. prove that $cos(2\\mathbf{u}, \\mathbf{v}) = cos(\\mathbf{u}, \\mathbf{v})$.\n",
    "2. Could the cosine similarity be negative when using *tf.idf* vector representations? Explain your answer.\n",
    "3. Could the cosine similarity be negative when using prediction-based, dense vector representations? Explain your answer.\n",
    "\n",
    "\n",
    "It is important that math is formatted appropriately, e.g. $x_i$ looks good, xi looks bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Solution 1 goes here.</font>\n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\cos(u, v) &= {u \\cdot v \\over \\lVert u \\rVert \\lVert v \\rVert} \\\\\\\\\n",
    "    \\cos(2 u, v) &= {(2 u) \\cdot v \\over \\lVert 2 u \\rVert \\lVert v \\rVert} \\\\\\\\\n",
    "    \\cos(2 u, v) &= {2 ( u \\cdot v ) \\over 2 \\lVert  u \\rVert \\lVert v \\rVert} \\\\\\\\\n",
    "    \\cos(2 u, v) &= {u \\cdot v \\over \\lVert u \\rVert \\lVert v \\rVert} \\\\\\\\\n",
    "    \\cos(2 u, v) &= \\cos(u, v)\n",
    "\\end{aligned}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Solution 2 goes here.</font>\n",
    "\n",
    "No. Both TF and IDF are based on counts of words, which cannot be negative. Since the vectors are made up of entirely positive numbers, they must fall in the same quadrant, and thus cannot be more than 90 degrees separate, corresponding to a dot product of 0. Therefore, the cosine similarity must be greater than or equal to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Solution 3 goes here.</font>\n",
    "\n",
    "Yes. Learned embeddings can contain negative numbers, which means they can point in any direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase embeddings (20p)\n",
    "\n",
    "Given a phrase consisting of a sequence of M words, $phrase = [word_1, word_2, ..., word_M]$, and given that we have already trained word embeddings $E(word)$ for all the words $word \\in V$ in the vocabulary, a simple way of creating an embedding for the phrase is by summing up the embeddings of its words:\n",
    "\\begin{equation}\n",
    "  E(phrase) = \\sum_{m = 1}^M E(word_m)\n",
    "\\end{equation}\n",
    "Considering an entire movie review to be a very long phrase, we could then train a binary logistic regression model with parameters $\\mathbf{w}$ and $b$ for sentiment classification. In that case, the larger the logit score $z(phrase) = \\mathbf{w}^T E(phrase) + b$, the higher the probability the model assigns to the positive sentiment for this $phrase$. Prove that in this approach, irrespective of the model parameters, the inequalities below cannot both hold:\n",
    "\\begin{aligned}\n",
    "  z(good) & > & z(not \\; good) \\\\\n",
    "  z(bad) & < & z(not \\; bad)\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">YOUR SOLUTION goes here.</font>\n",
    "\n",
    "First, we expand the equation for z(not good)\n",
    "\\begin{aligned}\n",
    "    E(not \\; good) &= E(not) + E(good) \\\\\\\\\n",
    "    z(not \\; good) &= w^T \\big(E(not) + E(good)\\big) + b \\\\\\\\\n",
    "    z(not \\; good) &= w^T E(not) + w^T E(good) + b\n",
    "\\end{aligned}\n",
    "\n",
    "Then, compare it to z(good)\n",
    "\n",
    "\\begin{aligned}\n",
    "    w^T E(good) + b &> w^T E(not) + w^T E(good) + b \\\\\\\\\n",
    "    \\cancel{w^T E(good) + b} &> w^T E(not) + \\cancel{w^T E(good) + b} \\\\\\\\\n",
    "    w^T E(not) &< 0\n",
    "\\end{aligned}\n",
    "\n",
    "If we repeat the process for the second inequality:\n",
    "\n",
    "\\begin{aligned}\n",
    "    w^T E(bad) + b &< w^T E(not) + w^T E(bad) + b \\\\\\\\\n",
    "    \\cancel{w^T E(bad) + b} &< w^T E(not) + \\cancel{w^T E(bad) + b} \\\\\\\\\n",
    "    w^T E(not) &> 0\n",
    "\\end{aligned}\n",
    "\n",
    "It is clear that $\\mathbf{w}^T E(not)$ cannot be both greater than 0 and less than 0 at the same time, so the inequalities cannot hold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time and memory complexity (bonus 20p)\n",
    "\n",
    "1. Describe an **efficient** procedure (pseudocode) for computing the *tf.idf* vectors for all the words in a vocabulary $V$, given a set of documents $D$ that contain a total of $N$ word occurrences, and a context window of size $C$. Compute its time and memory complexity, as a function of the size of $V$, $D$, $N$, and $C$.\n",
    "2. What are the time and memory complexity of the skip-gram word2vec model described in class for learning dense word embeddings? Assume the vocabulary is $V$, the corpus is a sequence of words of length $N$, the context window contains $C$ words, and that for every context word we sample $K$ negative words. Assume a gradient descent update is made for each center (target) word, and that the algorithm runs $E$ passes over the entire corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">YOUR SOLUTION goes here.</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All required import statements are here.\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "import operator\n",
    "import gzip\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy as sp\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Count-Based Word Vectors\n",
    "\n",
    "Most word vector models start from the following idea:\n",
    "\n",
    "*You shall know a word by the company it keeps ([Firth, J. R. 1957:11](https://en.wikipedia.org/wiki/John_Rupert_Firth))*\n",
    "\n",
    "Many word vector implementations are driven by the idea that similar words, i.e., (near) synonyms, will be used in similar contexts. As a result, similar words will often be spoken or written along with a shared subset of words, i.e., contexts. By examining these contexts, we can try to develop embeddings for our words. With this intuition in mind, many \"old school\" approaches to constructing word vectors relied on word counts.\n",
    "\n",
    "This part explores distributional similiarity in a dataset of 10,000 Wikipedia articles (4.4M words), building high-dimensional, sparse representations for words from the distinct contexts they appear in.  These representations allow for analysis of the most similar words to a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use as context 4 words to the left and 4 words to the right.\n",
    "window = 4\n",
    "\n",
    "# We will only consider as context the most common 10,000 words in the vocabulary.\n",
    "vocabSize = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1: Load corpus and create document frequency  dictionary (10p)\n",
    "\n",
    "Load the data from the Wikipedia file. Each line contains a Wikipedia document. After running this code, `wiki_data` should contain a list of all lowercased tokens in the corpus that contain only letters, whereas `dfs` should be a dictionary that maps each unique token to the number of Wikipedia documents in which the token appears (i.e. its document frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents: 10000\n"
     ]
    }
   ],
   "source": [
    "filename = \"../data/wiki.10K.txt\"\n",
    "\n",
    "dfs = defaultdict(int)\n",
    "Ndocs = 0\n",
    "wiki_data = []\n",
    "with open(filename, 'r', encoding = \"utf-8\") as fwiki:\n",
    "    for line in fwiki:\n",
    "        tokens = [t for t in line.lower().split() if t.isalpha()]\n",
    "        # YOUR CODE HERE\n",
    "        Ndocs += 1\n",
    "        for token in tokens:\n",
    "            dfs[token] += 1\n",
    "\n",
    "    wiki_data = dfs.keys()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Total number of documents:', Ndocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 276949), ('of', 130894), ('and', 114971), ('in', 111606), ('a', 81084), ('to', 78855), ('was', 47454), ('is', 39225), ('for', 32775), ('on', 31503), ('as', 30837), ('by', 27868), ('with', 27804), ('he', 25348), ('that', 22373), ('at', 21226), ('from', 20166), ('his', 19773), ('it', 18620), ('an', 15080)]\n"
     ]
    }
   ],
   "source": [
    "# Let's print the 20 tokens with the largest document frequency.\n",
    "top = sorted(dfs.items(), key = lambda item: item[1], reverse = True)\n",
    "print(top[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create empty word representations\n",
    "\n",
    "For each of the `vocabSize` most common words in the `data`, create a word representation that is initialized to an empty dictionary. Return the `word_representations` as a dictionary mapping each word to its empty word representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_representations(data, vocabSize):\n",
    "    word_representations = {}\n",
    "    vocab = Counter()\n",
    "    for i, word in enumerate(data):\n",
    "        vocab[word] += 1\n",
    "\n",
    "    topK = [k for k,v in vocab.most_common(vocabSize)]\n",
    "    for k in topK:\n",
    "        word_representations[k] = defaultdict(float)\n",
    "    return word_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_representations = create_representations(wiki_data, vocabSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'falcon'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mword_representations\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfalcon\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[31mKeyError\u001b[39m: 'falcon'"
     ]
    }
   ],
   "source": [
    "print(word_representations['falcon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update word representations\n",
    "\n",
    "Traverse the `data` from left to right, and for each word occurrence update its word representation by incrementing the counts for words appearing `window` words to the left and `window` words to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_representations(data, word_representations, window):\n",
    "    for i, word in enumerate(data):\n",
    "        if word not in word_representations:\n",
    "            continue\n",
    "        start = i - window if i - window > 0 else 0\n",
    "        end = i + window + 1 if i + window + 1 < len(data) else len(data)\n",
    "        for j in range(start, end):\n",
    "            if i != j:\n",
    "                word_representations[word][data[j]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict_keys' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mupdate_representations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwiki_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_representations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mupdate_representations\u001b[39m\u001b[34m(data, word_representations, window)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start, end):\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != j:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m         word_representations[word][\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m] += \u001b[32m1\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: 'dict_keys' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "update_representations(wiki_data, word_representations, window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_representations['falcon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize word representations\n",
    "\n",
    "Currently, `word_representations` is a dictionary that maps words to their representation, where each representation is itself a dictionary mapping a words to counts. This dictionary can be seen as a sparse vector representation, where each position in the vector corresponds to a word in the vocabulary. Since most words woudl have a co-occurrence count of 0, the dictionary only stores words for whic the counts are greater than 0.\n",
    "\n",
    "In the function below, we normalize each word representation such that its norm is 1. This is done by first computing the norm of the representation, and then dividing each count in the vector by the norm. We do this normalization such that dot-product between normalzied vectors is equivalent with cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(word_representations):\n",
    "    for word in word_representations:\n",
    "        total = 0\n",
    "        for key in word_representations[word]:\n",
    "            total += word_representations[word][key] * word_representations[word][key]\n",
    "            \n",
    "        total = math.sqrt(total)\n",
    "        for key in word_representations[word]:\n",
    "            word_representations[word][key] /= total    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize(word_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_representations['falcon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2: Cosine Similarity (10p)\n",
    "\n",
    "Write a function `dictionary_dot_product(dict1, dict2)` that computes the dot-product between the two normalized word representations stored as dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_dot_product(dict1, dict2):\n",
    "    dot = 0\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary_dot_product(word_representations['falcon'], word_representations['bird']))\n",
    "print(dictionary_dot_product(word_representations['falcon'], word_representations['bridge']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nearest Neighbors\n",
    "\n",
    "Write a function `find_nearest_neighbors(word_representations, query, K)` that takes as input a query word and returns the K most similar words.\n",
    "\n",
    "We first define a helper function `compute_sim(word_representations, query)` that takes as input a query word and computes, for each word in the vocabulary, the cosine similarity between the representation of that word and the representation of the query word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sim(word_representations, query):\n",
    "    if query not in word_representations:\n",
    "        print(\"'%s' is not in vocabulary\" % query)\n",
    "        return None\n",
    "    \n",
    "    scores = {}\n",
    "    for word in word_representations:\n",
    "        cosine = dictionary_dot_product(word_representations[query], word_representations[word])\n",
    "        scores[word] = cosine\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the K words with highest cosine similarity to a query in a set of word_representations\n",
    "def find_nearest_neighbors(word_representations, query, K):\n",
    "    scores = compute_sim(word_representations, query)\n",
    "    if scores != None:\n",
    "        sorted_x = sorted(scores.items(), key = operator.itemgetter(1), reverse = True)\n",
    "        for idx, (k, v) in enumerate(sorted_x[:K]):\n",
    "            print(\"%s\\t%s\\t%.5f\" % (idx, k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_nearest_neighbors(word_representations, \"falcon\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_nearest_neighbors(word_representations, \"musician\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2: Implement Tf.Idf Representation (10p)\n",
    "\n",
    "Q1: Fill out a function `tfidf` below.  This function takes as input a dict of word_representations and for each context word in `word_representations[word]` replaces its *count* value with its tf-idf score.  Use $\\log{(count + 1)}$ for tf and $\\log{N \\over df}$ for idf. This function should modify `word_representations` in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(word_representations):\n",
    "    for word in word_representations:\n",
    "        for key in word_representations[word]:\n",
    "            # YOUR CODE HERE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_word_representations = create_representations(wiki_data, vocabSize)\n",
    "update_representations(wiki_data, tf_idf_word_representations, window)\n",
    "tfidf(tf_idf_word_representations)\n",
    "normalize(tf_idf_word_representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3: Compare Count Representations with Tf.Idf Representations (10p)\n",
    "\n",
    "How does the tf.idf representation change the the nearest neighbors? Use `find_nearest_neighbors` on some of the words below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"falcon\" # \"musician\" student\" \"education\" \"bacteria\" \"beer\" \"brook\" \"greedy\" \"carbon\" \"prisoner\" \"river\" \"mountain\" \"germany\" \"child\" \"computer\" \"actor\" \"science\"\n",
    "find_nearest_neighbors(word_representations, query, 10)\n",
    "print()\n",
    "find_nearest_neighbors(tf_idf_word_representations, query, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">YOUR ANSWER goes here.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Prediction-Based Word Vectors\n",
    "\n",
    "As discussed in class, prediction-based dense word embeddings have come to dominate NLP. Here, we shall explore the embeddings produced by 'word2vec'. Please revisit the class notes and lecture slides for more details on the word2vec algorithm. If you're feeling adventurous, challenge yourself and try reading the [original paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf).\n",
    "\n",
    "Then run the following cells to load the word2vec vectors into memory. **Note**: This might take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "wv_from_bin = KeyedVectors.load_word2vec_format('../data/word2vec-google-news-300.gz', binary = True)\n",
    "vocab = list(wv_from_bin.key_to_index.keys())\n",
    "print(\"Loaded vocab size %i\" % len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1: Compare Word2Vec Embeddings with Co-occurrence Embeddings (10p)\n",
    "\n",
    "Let's use the word2vec embeddings to find the most similar words, usign the same targets as in part 1 above. Compare the quality of the top 10 words using word2vec with the top 10 most similar words from part 1 above. Which method is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_from_bin.most_similar(\"falcon\") # \"brook\", \"musician\" student\" beer\" education\" \"bacteria\" \"brook\" \"greedy\" \"carbon\" \"prisoner\" \"river\" \"mountain\" \"germany\" \"child\" \"computer\" \"actor\" \"science\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">YOUR ANSWER goes here.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing dimensionality of Word2Vec Word Embeddings\n",
    "\n",
    "1. Put the 3 million word2vec vectors into a matrix M\n",
    "2. Run reduce_to_k_dim (your Truncated SVD function) to reduce the vectors from 300-dimensional to 2-dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we construct a method that performs dimensionality reduction on the matrix to produce k-dimensional embeddings. We use SVD to take the top k components and produce a new matrix of k-dimensional embeddings. \n",
    "\n",
    "**Note:** All of numpy, scipy, and scikit-learn (`sklearn`) provide *some* implementation of SVD, but only scipy and sklearn provide an implementation of Truncated SVD, and only sklearn provides an efficient randomized algorithm for calculating large-scale Truncated SVD. So please use [sklearn.decomposition.TruncatedSVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_to_k_dim(M, k = 2):\n",
    "    \"\"\" Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)\n",
    "        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:\n",
    "            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "    \n",
    "        Params:\n",
    "            M (numpy matrix of shape (number of corpus words, number of corpus words)): co-occurence matrix of word counts\n",
    "            k (int): embedding size of each word after dimension reduction\n",
    "        Return:\n",
    "            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.\n",
    "                    In terms of the SVD from math class, this actually returns U * S\n",
    "    \"\"\"    \n",
    "    n_iters = 10     # Use this parameter in your call to `TruncatedSVD`\n",
    "    M_reduced = None\n",
    "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
    "    \n",
    "    # ------------------\n",
    "    svd = TruncatedSVD(n_components = k, n_iter = n_iters, random_state=42)\n",
    "    svd.fit(M)\n",
    "    M_reduced = M @ svd.components_.T\n",
    "    # ------------------\n",
    "\n",
    "    print(\"Done.\")\n",
    "    \n",
    "    return M_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix_of_vectors(wv_from_bin, required_words = []):\n",
    "    \"\"\" Put the word2vec vectors into a matrix M.\n",
    "        Param:\n",
    "            wv_from_bin: KeyedVectors object; the 3 million word2vec vectors loaded from file\n",
    "        Return:\n",
    "            M: numpy matrix shape (num words, 300) containing the vectors\n",
    "            word2Ind: dictionary mapping each word to its row number in M\n",
    "    \"\"\"\n",
    "    import random\n",
    "    words = list(wv_from_bin.index_to_key)\n",
    "    print(\"Shuffling words ...\")\n",
    "    random.shuffle(words)\n",
    "    words = words[:10000]\n",
    "    print(\"Putting %i words into word2Ind and matrix M...\" % len(words))\n",
    "    word2Ind = {}\n",
    "    M = []\n",
    "    curInd = 0\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(wv_from_bin.get_vector(w))\n",
    "            word2Ind[w] = curInd\n",
    "            curInd += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    for w in required_words:\n",
    "        try:\n",
    "            M.append(wv_from_bin.get_vector(w))\n",
    "            word2Ind[w] = curInd\n",
    "            curInd += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    M = np.stack(M)\n",
    "    print(\"Done.\")\n",
    "    return M, word2Ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2: Word2Vec Plot Analysis (10p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we write a function to plot a set of 2D vectors in 2D space. For graphs, we will use Matplotlib (`plt`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(M_reduced, word2Ind, words):\n",
    "    \"\"\" Plot in a scatterplot the embeddings of the words specified in the list \"words\".\n",
    "        NOTE: do not plot all the words listed in M_reduced / word2Ind.\n",
    "        Include a label next to each point.\n",
    "        \n",
    "        Params:\n",
    "            M_reduced (numpy matrix of shape (number of unique words in the corpus , k)): matrix of k-dimensional word embeddings\n",
    "            word2Ind (dict): dictionary that maps word to indices for matrix M\n",
    "            words (list of strings): words whose embeddings we want to visualize\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------\n",
    "    xvals = []\n",
    "    yvals = []\n",
    "    for word in words:\n",
    "        embed2D = M_reduced[word2Ind[word]]\n",
    "        xvals.append(embed2D[0])\n",
    "        yvals.append(embed2D[1])\n",
    "        \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(xvals, yvals)\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        ax.annotate(word, (xvals[i], yvals[i]))\n",
    "    # ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to plot the 2D word2vec embeddings for `['music', 'jazz', 'opera', 'paris', 'berlin', 'tokyo', 'queen', 'king', 'prince', 'volcano', 'chemistry', 'biology', 'physics', 'lava', 'sonata']`.\n",
    "\n",
    "What clusters together in 2-dimensional embedding space? What doesn't cluster together that you might think should have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# Run this code to Reduce 300-Dimensional Word Embeddings to k Dimensions\n",
    "# Note: This may take several minutes\n",
    "# -----------------------------------------------------------------\n",
    "words = ['music', 'jazz', 'opera', 'paris', 'berlin', 'tokyo', 'queen', 'king', 'prince', 'volcano', 'chemistry', 'biology', 'physics', 'lava', 'sonata']\n",
    "M, word2Ind = get_matrix_of_vectors(wv_from_bin, required_words = words)\n",
    "M_reduced = reduce_to_k_dim(M, k = 2)\n",
    "\n",
    "plot_embeddings(M_reduced, word2Ind, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">YOUR ANSWER goes here.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "Now that we have word vectors, we need a way to quantify the similarity between individual words, according to these vectors. One such metric is cosine-similarity. We will be using this to find words that are \"close\" and \"far\" from one another.\n",
    "\n",
    "We can think of n-dimensional vectors as points in n-dimensional space. If we take this perspective L1 and L2 Distances help quantify the amount of space \"we must travel\" to get between these two points. Another approach is to examine the angle between two vectors. From trigonometry we know that:\n",
    "\n",
    "<img src=\"imgs/inner_product.png\" width=20% style=\"float: center;\"></img>\n",
    "\n",
    "Instead of computing the actual angle, we can leave the similarity in terms of $similarity = cos(\\Theta)$. Formally the [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) $s$ between two vectors $p$ and $q$ is defined as:\n",
    "\n",
    "$$s = \\frac{p \\cdot q}{||p|| ||q||}, \\textrm{ where } s \\in [-1, 1] $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3: Polysemous Words [code + written] (10p)\n",
    "\n",
    "Find a [polysemous](https://en.wikipedia.org/wiki/Polysemy) word (for example, \"leaves\" or \"scoop\") such that the top-10 most similar words (according to cosine similarity) contains related words from *both* meanings. For example, \"leaves\" has both \"vanishes\" and \"stalks\" in the top 10, and \"scoop\" has both \"handed_waffle_cone\" and \"lowdown\". You will probably need to try several polysemous words before you find one. Please state the polysemous word you discover and the multiple meanings that occur in the top 10. Why do you think many of the polysemous words you tried didn't work?\n",
    "\n",
    "**Note**: You should use the `wv_from_bin.most_similar(word)` function to get the top 10 similar words. This function ranks all other words in the vocabulary with respect to their cosine similarity to the given word. For further assistance please check the __[GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Write your polysemous word exploration code here.\n",
    "\n",
    "wv_from_bin.most_similar(\"leaves\")\n",
    "\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">YOUR ANSWER goes here.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.4: Synonyms & Antonyms (10p)\n",
    "\n",
    "When considering Cosine Similarity, it's often more convenient to think of Cosine Distance, which is simply 1 - Cosine Similarity.\n",
    "\n",
    "Find three words (w1,w2,w3) where w1 and w2 are synonyms and w1 and w3 are antonyms, but Cosine Distance(w1,w3) < Cosine Distance(w1,w2). For example, w1 = \"happy\" is closer to w3 = \"sad\" than to w2 = \"cheerful\". \n",
    "\n",
    "Once you have found your example, please give a possible explanation for why this counter-intuitive result may have happened.\n",
    "\n",
    "You should use the the `wv_from_bin.distance(w1, w2)` function here in order to compute the cosine distance between two words. Please see the __[GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html)__ for further assistance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Write your synonym & antonym exploration code here.\n",
    "\n",
    "w1 = \"happy\"\n",
    "w2 = \"cheerful\"\n",
    "w3 = \"sad\"\n",
    "w1_w2_dist = wv_from_bin.distance(w1, w2)\n",
    "w1_w3_dist = wv_from_bin.distance(w1, w3)\n",
    "\n",
    "print(\"Synonyms {}, {} have cosine distance: {}\".format(w1, w2, w1_w2_dist))\n",
    "print(\"Antonyms {}, {} have cosine distance: {}\".format(w1, w3, w1_w3_dist))\n",
    "\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">YOUR ANSWER goes here.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving Analogies with Word Vectors\n",
    "Word2Vec vectors have been shown to *sometimes* exhibit the ability to solve analogies. \n",
    "\n",
    "As an example, for the analogy \"man : king :: woman : x\", what is x?\n",
    "\n",
    "In the cell below, we show you how to use word vectors to find x. The `most_similar` function finds words that are most similar to the words in the `positive` list and most dissimilar from the words in the `negative` list. The answer to the analogy will be the word ranked most similar (largest numerical value).\n",
    "\n",
    "**Note:** Further Documentation on the `most_similar` function can be found within the __[GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to answer the analogy -- man : king :: woman : x\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'king'], negative=['man']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.5: Finding Analogies (10p)\n",
    "\n",
    "Find 5 examples of analogies that holds according to these vectors (i.e. the intended word is ranked top). In your solution please state the full analogy in the form x:y :: a:b. If you believe the analogy is complicated, explain why the analogy holds in one or two sentences. \n",
    "\n",
    "**Note**: You may have to try many analogies to find ones that work!\n",
    "\n",
    "Document also 5 examples of analogies that do not hold according to the learned word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Write your analogy exploration code here.\n",
    "\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=[], negative=[]))\n",
    "\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">YOUR ANSWER goes here.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.6: Guided Analysis of Bias in Word Vectors (10p)\n",
    "\n",
    "It's important to be cognizant of the biases (gender, race, sexual orientation etc.) implicit to our word embeddings.\n",
    "\n",
    "Run the cell below, to examine (a) which terms are most similar to \"woman\" and \"boss\" and most dissimilar to \"man\", and (b) which terms are most similar to \"man\" and \"boss\" and most dissimilar to \"woman\". What do you find in the top 10?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "# Here `positive` indicates the list of words to be similar to and `negative` indicates the list of words to be\n",
    "# most dissimilar from.\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'boss'], negative=['man']))\n",
    "print()\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=['man', 'boss'], negative=['woman']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">YOUR ANSWER goes here.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.7: Independent Analysis of Bias in Word Vectors (10p)\n",
    "\n",
    "Use the `most_similar` function to find at least 2 other cases where some bias is exhibited by the vectors. Please briefly explain the type of bias that you discover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Write your bias exploration code here.\n",
    "\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=[], negative=[]))\n",
    "print()\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=[,], negative=[]))\n",
    "\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">YOUR ANSWER goes here.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.8: Thinking About Bias (10p)\n",
    "\n",
    "What might be the cause of these biases in the word vectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">YOUR ANSWER goes here.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Sentiment Analysis using LLM Embeddings\n",
    "\n",
    "In this part of the assignment, you will use an LLM's embeddings API, such as [Gemini Embeddings](https://ai.google.dev/gemini-api/docs/embeddings), to map movie reviews into embeddings, and then use these embeddigns as feature vector to train a sentiment analyzer. Follow the sequence of steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing (50 points)\n",
    "\n",
    "1. Read the training examples from `imdb_sentiment_train.txt`. Store the labels in an array `y_train` and the corresponding movie reviews in the list of strings `x_train`.\n",
    "\n",
    "2. Pass the list of reviews `x_train` to the Gemini Embeddings API and store the resulting embeddings matrix in a NumPy array `embeddings_train`. Use a size of 512 for the embeddings.\n",
    "\n",
    "3. Save the `embeddings_train` array in `../data/embeddings.train.npy` using the `numpy.save()` function. Save the labels `y_train` into `../data/labels.train.txt`.\n",
    "\n",
    "4. Repeat steps 1, 2, and 3 above for the test examples in `imdb_sentiment_test.txt` and create the corresponding `y_test`, `embeddings_test`, and the files `../data/embeddings.test.npy` and `../data/labels.test.txt`.\n",
    "\n",
    "It is recommended that you split the examples into batches, e.g., each batch containts 100 examples, then provide each batch of examples to the Gemini API, which will return a batch of embeddings. If you use an API key associated with the $300 free credits, this should not incur any rate-limit issues. If you use a generic, free Gemini account, you may want to insert a 10s delay between batches.\n",
    "\n",
    "For your convenience, if you still encounter issues, we created and saved the Gemini embeddings in the `../data' folder. Nevertheless, submit your code and the output cell you get from it showing any issues you entounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from google import genai\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Model Training and Testing (50 points)\n",
    "\n",
    "5. Read the training embeddings from `../data/embeddings.train.npy` into a NumPy array `x_train`, and read the training labels from `../data/labels.train.txt` into `y_train`.\n",
    "\n",
    "6. Train a binary classifier on `x_train` and `y_train`, using the `sklearn.linear_model.LogisticRegression` class.\n",
    "\n",
    "7. Read the test embeddings from `../data/embeddings.test.npy` into a NumPy array `x_test`, and read the training labels from `../data/labels.test.txt` into `y_test`.\n",
    "\n",
    "8. Use the trained classifier to predict the labels of the test embeddings in `x_text` and store the predictions in `pred_test`.\n",
    "\n",
    "9. Compute and report the test accuracy by comparing the labels in `y_test` and `pred_test`.\n",
    "\n",
    "10. Compare this accuracy with the accuracy obtained at homework 3.\n",
    "\n",
    "Our trained logistic regression model obtained **96.4% test accuracy**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression(penalty = 'l2', C = 1.0, solver = 'lbfgs', max_iter = 1000)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Comparison of LR with Embeddings vs. LR with Engineered features.</font>\n",
    "\n",
    "YOUR ANALYSIS GOES HERE:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation: Bonus points\n",
    "\n",
    "Anything extra goes here. For example:\n",
    "\n",
    "1. (10p) How does changing the window size (smaller, larger) change the word to word similarities?\n",
    "2. (10p) Even though `count_unigram_context` computes count-based vector representations only for the top K (10000) most common words in the vocabulary, it uses all the words that appear in the context. Change it to only use word in the context that are in the top K most common words, and see if it improves the results.\n",
    "3. (40p) Use the development examples in `../data/imdb_sentiment_dev.txt` to tune the hyperparameters of the LogisticRegression model, e.g., by usign the K-fold cross-validation function from sklearn. Report how much this improves performance on the test examples.\n",
    "4. (20p) Tune the embedding size (from 256 to 3072) on the development data and report which one obtains best performance on development, then use it to train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
