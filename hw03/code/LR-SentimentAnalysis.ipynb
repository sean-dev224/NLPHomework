{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression and Sentiment analysis \n",
    "\n",
    "In this assignment you will implement and experiment with various feature engineering techniques in the context of Logistic Regression models for Sentiment classification of movie reviews.\n",
    "\n",
    "1. Logistic regression, linear algebra, and derivatives.\n",
    "\n",
    "2. Read lexicons of positive and negative sentiment words.\n",
    "\n",
    "3. Implement overall positive and negative lexicon count features.\n",
    "\n",
    "4. Implement per-lexicon-word count features.\n",
    "\n",
    "5. Implement document length feature.\n",
    "\n",
    "6. Implement deictic features.\n",
    "\n",
    "7. Pre-processing for negation.\n",
    "\n",
    "8. Plot learning curves.\n",
    "\n",
    "9. Open bonus points.\n",
    "\n",
    "10. Analysis of results.\n",
    "\n",
    "We will use the LR model implemented in sklearn:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sean Devlin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\"> Submission Instructions</font>\n",
    "\n",
    "1. Click the Save button at the top of the Jupyter Notebook.\n",
    "2. Please make sure to have entered your name above.\n",
    "3. Select Cell -> All Output -> Clear. This will clear all the outputs from all cells (but will keep the content of ll cells). \n",
    "4. Select Cell -> Run All. This will run all the cells in order, and will take several minutes.\n",
    "5. Once you've rerun everything, select File -> Download as -> PDF via LaTeX and download a PDF version *wikipedia.pdf* showing the code and the output of all cells, and save it in the same folder that contains the notebook file *wikipedia.ipynb*.\n",
    "6. Look at the PDF file and make sure all your solutions are there, displayed correctly. The PDF is the only thing we will see when grading!\n",
    "7. Submit **both** your PDF and notebook on Canvas.\n",
    "8. Make sure your your Canvas submission contains the correct files by downloading it after posting it on Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory (10p + 5p + 5p + 5p + 5p)\n",
    "\n",
    "1. Let $\\mathbf{w} = [0.3, -0.4]$, $\\mathbf{x} = [1.2, 0.5]$, and $b = -0.66$. Compute the following:\n",
    "    - The Euclidean (L2) norms $||\\mathbf{w}||$ and $||\\mathbf{x}||$.\n",
    "    - The logit score $z = \\mathbf{w}^T \\mathbf{x} + b$.\n",
    "    - The probability $p(y = 1 | \\mathbf{x})$ that $\\mathbf{x}$ is positive, as computed by a binary logistic regression model with parameters $\\mathbf{w}, b$.\n",
    "    - Keeping $\\mathbf{w}$ and $\\mathbf{x}$ fixed, find the value of $b$ such that the probability above becomes 0.5.\n",
    "\n",
    "2. Prove that the derivative of the sigmoid function can be written as $\\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z))$.\n",
    "\n",
    "3. Using the chain rule of differentiation, prove that $\\displaystyle\\frac{\\delta \\sigma(\\mathbf{w}^T\\mathbf{x} +b)}{\\delta \\mathbf{w}} = \\sigma(\\mathbf{w}^T\\mathbf{x} +b) \\cdot (1 - \\sigma(\\mathbf{w}^T\\mathbf{x} +b)) \\cdot \\mathbf{x}$.\n",
    "\n",
    "4. Subtracting a constant $c$ from all logit score does not change the probabilities computed by\n",
    "softmax, i.e., $softmax(z_1 − c, z_2 − c, ..., z_K − c) = softmax(z_1, z_2, ..., z_K)$\n",
    "\n",
    "5. Show the gradient of the objective function $J(w_1, w_2) = 2w_1^2 + 3w_2^2 -4w_1 +12w_2 + 15$. Use the gradient the parameters $w_1$ and $w_2$ that minimize $J$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ||w|| = 0.5    ||x|| = 1.3\n",
    "\n",
    "2. \n",
    "\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From documents to feature vectors\n",
    "This section illustratess the prototypical components of machine learning pipeline for an NLP task, in this case document classification:\n",
    "\n",
    "1. Read document examples (train, devel, test) from files with a predefined format:\n",
    "    - assume one document per line, usign the format \"\\<label\\> \\<text\\>\".\n",
    "\n",
    "2. Tokenize each document:\n",
    "    - using a spaCy tokenizer.\n",
    "\n",
    "3. Feature extractors:\n",
    "    - so far, just words.\n",
    "\n",
    "4. Process each document into a feature vector:\n",
    "    - map document to a dictionary of feature names.\n",
    "    - map feature names to unique feature IDs.\n",
    "    - each document is a feature vector, where each feature ID is mapped to a feature value (e.g. word occurences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from scipy import sparse\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spaCy tokenizer.\n",
    "spacy_nlp = English()\n",
    "\n",
    "def spacy_tokenizer(text):\n",
    "    tokens = spacy_nlp.tokenizer(text)\n",
    "    \n",
    "    return [token.text for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_examples(filename):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(filename, mode = 'r', encoding = 'utf-8') as file:\n",
    "        for line in file:\n",
    "            [label, text] = line.rstrip().split(' ', maxsplit = 1)\n",
    "            X.append(text)\n",
    "            Y.append(label)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_features(tokens):\n",
    "    feats = {}\n",
    "    for word in tokens:\n",
    "        feat = 'WORD_%s' % word\n",
    "        if feat in feats:\n",
    "            feats[feat] +=1\n",
    "        else:\n",
    "            feats[feat] = 1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(feats, new_feats):\n",
    "    for feat in new_feats:\n",
    "        if feat in feats:\n",
    "            feats[feat] += new_feats[feat]\n",
    "        else:\n",
    "            feats[feat] = new_feats[feat]\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function tokenizes the document, runs all the feature extractors on it and assembles the extracted features into a dictionary mapping feature names to feature values. It is important that feature names do not conflict with each other, i.e. **different features should have different names**. Each document will have its own dictionary of features and their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs2features(trainX, feature_functions, tokenizer):\n",
    "    examples = []\n",
    "    count = 0\n",
    "    for doc in trainX:\n",
    "        feats = {}\n",
    "\n",
    "        tokens = tokenizer(doc)\n",
    "        \n",
    "        for func in feature_functions:\n",
    "            add_features(feats, func(tokens))\n",
    "\n",
    "        examples.append(feats)\n",
    "        count +=1\n",
    "        \n",
    "        if count % 100 == 0:\n",
    "            print('Processed %d examples into features' % len(examples))\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper function converts feature names to unique numerical IDs.\n",
    "\n",
    "def create_vocab(examples):\n",
    "    feature_vocab = {}\n",
    "    idx = 0\n",
    "    for example in examples:\n",
    "        for feat in example:\n",
    "            if feat not in feature_vocab:\n",
    "                feature_vocab[feat] = idx\n",
    "                idx += 1\n",
    "                \n",
    "    return feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper function converts a set of examples from a dictionary of feature names to values representation\n",
    "# to a sparse representation of feature ids to values. This is important because almost all feature values will\n",
    "# be 0 for most documents and it would be wasteful to save all in memory.\n",
    "\n",
    "def features_to_ids(examples, feature_vocab):\n",
    "    new_examples = sparse.lil_matrix((len(examples), len(feature_vocab)))\n",
    "    for idx, example in enumerate(examples):\n",
    "        for feat in example:\n",
    "            if feat in feature_vocab:\n",
    "                new_examples[idx, feature_vocab[feat]] = example[feat]\n",
    "                \n",
    "    return new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation pipeline for the Logistic Regression classifier.\n",
    "\n",
    "def train_and_test(trainX, trainY, devX, devY, feature_functions, tokenizer):\n",
    "    # Pre-process training documents. \n",
    "    trainX_feat = docs2features(trainX, feature_functions, tokenizer)\n",
    "\n",
    "    # Create vocabulary from features in training examples.\n",
    "    feature_vocab = create_vocab(trainX_feat)\n",
    "    print('Vocabulary size: %d' % len(feature_vocab))\n",
    "\n",
    "    trainX_ids = features_to_ids(trainX_feat, feature_vocab)\n",
    "    \n",
    "    # Train LR model.\n",
    "    lr_model = LogisticRegression(penalty = 'l2', C = 1.0, solver = 'lbfgs', max_iter = 1000)\n",
    "    lr_model.fit(trainX_ids, trainY)\n",
    "    \n",
    "    # Pre-process test documents. \n",
    "    devX_feat = docs2features(devX, feature_functions, tokenizer)\n",
    "    devX_ids = features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    # Test LR model.\n",
    "    print('Accuracy: %.3f' % lr_model.score(devX_ids, devY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 28692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdev/projects/NLPHomework/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Accuracy: 0.839\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "datapath = '../data'\n",
    "\n",
    "train_file = os.path.join(datapath, 'imdb_sentiment_train.txt')\n",
    "trainX, trainY = read_examples(train_file)\n",
    "\n",
    "dev_file = os.path.join(datapath, 'imdb_sentiment_dev.txt')\n",
    "devX, devY = read_examples(dev_file)\n",
    "\n",
    "# Specify features to use.\n",
    "features = [word_features]\n",
    "\n",
    "# Evaluate LR model.\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate LR model performance when adding positive and negative lexicon features. We will be using Bing Liu's sentiment lexicons from https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the positive and negative sentiment lexicons (10p)\n",
    "\n",
    "There should be 2006 entries in the positive lexicon and 4783 entries in the positive lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006 entries in the positive lexicon.\n",
      "4783 entries in the negative lexicon.\n"
     ]
    }
   ],
   "source": [
    "def read_lexicon(filename):\n",
    "    lexicon = set()\n",
    "    with open(filename, mode = 'r', encoding = 'ISO-8859-1') as file:\n",
    "        for line in file:\n",
    "            if line[0] != ';' and line[0] != '\\n':\n",
    "                lexicon.add(line)\n",
    "\n",
    "    \n",
    "    return lexicon\n",
    "\n",
    "lexicon_path = '../data/bliu'\n",
    "\n",
    "poslex_file = os.path.join(lexicon_path, 'positive-words.txt')\n",
    "neglex_file = os.path.join(lexicon_path, 'negative-words.txt')\n",
    "\n",
    "poslex = read_lexicon(poslex_file)\n",
    "neglex = read_lexicon(neglex_file)\n",
    "\n",
    "\n",
    "print(len(poslex), 'entries in the positive lexicon.')\n",
    "print(len(neglex), 'entries in the negative lexicon.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the lexicons to create two lexicon features (15p)\n",
    "\n",
    "- A feature 'POSLEX' whose value indicates how many tokens belong to the positive lexicon.\n",
    "- A feature 'NEGLEX' whose value indicates how many tokens belong to the negative lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_lexicon_features(tokens):\n",
    "    feats = {'POSLEX': 0, 'NEGLEX': 0}\n",
    "    # YOUR CODE HERE\n",
    "    for token in tokens:\n",
    "        if token in poslex:\n",
    "            feats['POSLEX'] += 1\n",
    "        if token in neglex:\n",
    "            feats['NEGLEX'] += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the LR model using the two new lexicon features. Expected accuracy is around 83.8%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 28694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdev/projects/NLPHomework/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Accuracy: 0.838\n"
     ]
    }
   ],
   "source": [
    "# Specify features to use.\n",
    "features = [word_features, two_lexicon_features]\n",
    "\n",
    "# Evaluate LR model.\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a separate feature for each word that appears in each lexicon (20p)\n",
    "\n",
    "- If a word from the positive lexicon (e.g. 'like') appears N times in the document (e.g. 5 times), add a positive lexicon feature 'POSLEX_word' for that word that is associated that value (e.g. {'POSLEX_like' : 5}.\n",
    "- Similarly, if a word from the negative lexicon (e.g. 'dislike') appears N times in the document (e.g. 5 times), add a negative lexicon feature 'NEGLEX_word' for that word that is associated that value (e.g. {'NEGLEX_dislike' : 5}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexicon_features(tokens):\n",
    "    feats = {}\n",
    "    # YOUR CODE HERE\n",
    "    # Assume the positive and negative lexicons are available in poslex and neglex, respectively.\n",
    "    for token in tokens:\n",
    "        feature = ''\n",
    "        if token in poslex:\n",
    "            feature = f\"POSLEX_{token}\"\n",
    "        if token in neglex:\n",
    "            feature = f\"NEGLEX_{token}\"\n",
    "\n",
    "        #update dictionary\n",
    "        if feature != '':\n",
    "            if feature in feats:\n",
    "                feats[feature] += 1\n",
    "            else:\n",
    "                feats[feature] = 1\n",
    "\n",
    "    \n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the LR model using the new per-lexicon word features. Expected accuracy is arpund 83.9%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 28692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdev/projects/NLPHomework/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Accuracy: 0.839\n"
     ]
    }
   ],
   "source": [
    "# Specify features to use.\n",
    "features = [word_features, lexicon_features]\n",
    "\n",
    "# Evaluate LR model.\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add document length feature (10p)\n",
    "\n",
    "Add a feature 'DOC_LEN' whose value is the natural logarithm of the document length (use *math.log* to compute logarithms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def len_feature(tokens):\n",
    "    feat = {'DOC_LEN': math.log(len(tokens))}\n",
    "\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the LR model using the new document length feature. Expected accuracy is around 84.0%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 28693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdev/projects/NLPHomework/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Accuracy: 0.839\n"
     ]
    }
   ],
   "source": [
    "# Specify features to use.\n",
    "features = [word_features, lexicon_features, len_feature]\n",
    "\n",
    "# Evaluate LR model.\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add deictic features (15p)\n",
    "\n",
    "Add a feature 'DEICTIC_COUNT' that counts the number of 1st and 2nd person pronouns in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deictic_feature(tokens):\n",
    "    pronouns = set(('i', 'my', 'me', 'we', 'us', 'our', 'you', 'your'))\n",
    "    count = 0\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    for token in tokens:\n",
    "        if token in pronouns:\n",
    "            count += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    return {'DEICTIC_COUNT': count}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the LR model using the deictic features. Expected accuracy is around 84.2%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 28694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdev/projects/NLPHomework/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Accuracy: 0.837\n"
     ]
    }
   ],
   "source": [
    "# Specify features to use.\n",
    "features = [word_features, lexicon_features, len_feature, deictic_feature]\n",
    "\n",
    "# Evaluate LR model.\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try without the word features. Expected accuracy is around 80.4%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 2\n",
      "Processed 100 examples into features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdev/projects/NLPHomework/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Accuracy: 0.503\n"
     ]
    }
   ],
   "source": [
    "# Specify features to use.\n",
    "features = [lexicon_features, len_feature, deictic_feature]\n",
    "\n",
    "# Evaluate LR model.\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing for negation (10p)\n",
    "\n",
    "Preprocess the tokens to account for negation, as explained on slide 36 in the LR lecture, and integrate in the model that uses `word_features`, `lexicon_features`, `len_feature`, and `deictic_feature`.\n",
    "\n",
    "- Pre-process the text for all negation words contained in the lexicon `../data/negation_words.txt`.\n",
    "- Need to rewrite the sentiment lexicon features such that whenever modified by a negation word, a positive sentiment word is counted as negative, i.e. 'NOT_like' will be a negative sentiment token. The prefix 'NOT_' should be added irrespective of the actual negative word used, e.g. 'not', 'never', etc.\n",
    "  * For bonus points, you can also run evaluations where the actual negative word is used as a prefix, e.g. 'never' before 'like' would lead to a feature called 'NEVER_like'.\n",
    "- Train and evaluate the performance of the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Bonus] Compute learning curve (15p)\n",
    "\n",
    "Select the best performing model and plot its accuracy vs. number of training examples. Vary the number of training examples by selecting for each class the first N examples in the file, where $N \\in \\{50, 100, 150, 250, 350, 450, 550, 650, 750\\}$. For example, the first 50 positive examples would be inb `X[:50]`, whereas the first 50 negative examples would be in `X[750:800]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus points ##\n",
    "Anything extra goes here. For example, can you do feature engineering or hyper-parameter tuning such that accuracy gets over 85%? The larger the gain in accuracy, the more bonus points awarded.\n",
    "- Evaluate the impact of other features, such as the presence of exclamation points, or replacing word features with lemma features.\n",
    "\n",
    "- Determine the importance of counts by using binary word features instead of count features, i.e. does the word appear or not in the document, instead of how many times.\n",
    "\n",
    "- Simple feature selection: use only features that appear at least K times in the training data (try K = 3, K = 5).\n",
    "  * Also evaluate the impact of feature selection when usign smaller valeus for the `C` hyper-parameter for L2 regularization.\n",
    "\n",
    "- Look at the mistakes the model made (error analysis) and see if you can design new features to address the more common mistakes.\n",
    "\n",
    "- Replace the spaCy tokenizer with the tiktoken BPE tokenizer and see impact on performance (accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis (10 + 10p)\n",
    "Include an analysis of the results that you obtained in the experiments above.\n",
    "\n",
    "**Error analysis**: Do some basic error analysis where you try to explain the mistakes that the best model made and provide ideas for possible features that would alleviate these mistakes.\n",
    "\n",
    "**Interpretability**: From each class of features take 2 features that you think should be strongly correlated with the positive or negative label, and determine if the model learned a corresponding parameter that correctly expresses this correlation. For example, the feature 'WORD_loved' is expected to be very correlated with the positive label, as such the model should learn a corresponding large positive weight.\n",
    "  - *Hint: for this, you may consider using the `coef_` attribute of the LogisticRegression class.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
